{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"<code>textnoisr</code>: Adding random noise to a dataset","text":"<p><code>textnoisr</code> is a python package that allows to add random noise to a text dataset, and to control very accurately the quality of the result.</p> <p>You can install it using <code>pip</code>:</p> <pre><code>pip install textnoisr\n</code></pre> <p>Here is an example if your dataset consists on the first few lines of the Zen of python:</p>  Raw text   Noisy text  <pre><code>The Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\n...\n</code></pre> <pre><code>TheO Zen of Python, by Tim Pfter\n\nBzeautiUful is ebtter than ugly.\nEqxplicin is better than imlicit.\nSimple is beateUr than comdplex.\nComplex is better than comwlicated.\nFlat is bejAter than neseed.\n...\n</code></pre> <p>Four types of \"actions\" are implemented:</p> <ul> <li>insert a random character, e.g.        STEAM  \u2192  STREAM,</li> <li>delete a random character, e.g.        STEAM  \u2192  TEAM,</li> <li>substitute a random character, e.g.    STEAM  \u2192  STEAL.</li> <li>swap two consecutive characters, e.g.  STEAM  \u2192  STEMA</li> </ul> <p>The general philosophy of the package is that only one single parameter is needed to control the noise level. This \"noise level\" is applied character-wise, and corresponds roughly to the probability for a character to be impacted.</p> <p>More precisely, this noise level is calibrated so that the Character Error Rate of a noised dataset converges to this value as the amount of text increases.</p> Why a whole package for such a simple task? <p>In the case of inserting, deleting and substituting characters at random with a probability \\(p\\), the Character Error Rate is only the average number of those operations, so it will converge to the input value \\(p\\) due to the Law of Large Numbers.</p> <p>However, the case of swapping consecutive characters is not trivial at all for two reasons:</p> <ul> <li> <p>First, swapping two characters is not an \"atomic operation\" with respect to the Character Error Rate metric.</p> </li> <li> <p>Second, we do not want to swap repeatedly the same character over and over again if the probability to apply the swap action is high: STEAM  \u2192  TSEAM TSEAM  \u2192  TESAM TESAM  \u2192  TEASM TEASM  \u2192  TEAMS This would be equivalent to STEAM  \u2192 TEAMS, so this cannot be considered \"swapping consecutive characters\". To avoid this behavior, we must avoid swapping a character if it has just been swapped. This breaks the independency between one character and the following one, and makes the Law of Large Numbers not applicable.</p> </li> </ul> <p>We use Markov Chains to model the swapping of characters. This allows us to compute and correct the corresponding bias in order to make it straightforward for the user to get the desired Character Error Rate, as if the Law of Large Number could be applied!</p> <p>All the details of this unbiasing are here. The goal of this package is for the user to be confident on the result without worrying about the implementation details.</p> <p>The documentation follows this plan:</p> <ul> <li>You may want to follow a quick tutorial to learn the basics of the package,</li> <li>The Results page illustrates how no calibration is needed in order to add noise to a corpus with a target Character Error Rate.</li> <li>The How this works section explains the mechanisms, and some design choices of this package. We have been extra careful to explain how some statistical bias have been avoided, for the package to be both user-friendly and correct. A dedicated page deeps dive in the case of the <code>swap</code> action.</li> <li>The API Reference details all the technical descriptions needed.</li> </ul> <p>There is also a Medium article about this project.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#textnoisr.noise","title":"<code>textnoisr.noise</code>","text":"<p>Add noise into text.</p>"},{"location":"api/#textnoisr.noise.CharNoiseAugmenter","title":"<code>CharNoiseAugmenter</code>","text":"<p>Add noise into text according to a noise level measured between 0 and 1.</p> <p>It will add noise to a string by modifying each character     according to a probability and a list of actions.     Possible actions are <code>insert</code>, <code>swap</code>, <code>substitute</code> and <code>delete</code>.</p> <p>For actions <code>insert</code> and <code>substitute</code>, new characters are drawn from <code>character_set</code>     which is the set of ascii letters (lower and upper) by default. The <code>swap</code> action swaps 2 consecutive characters,     but one character can not be swapped twice.     So if a pair of characters has been swapped,     we move to the next pair of characters.</p> <p>With enough samples, the CER of the output tends to the noise level     (terms and conditions may apply,     see details in docs/how_this_works.md).</p> <p>Parameters:</p> Name Type Description Default <code>noise_level</code> <code>float</code> <p>between 0 and 1, it corresponds to the level of noise to add to the text. In most cases (see details above for caveats), the Character Error Rate of the output will converge to this value. For <code>swap</code> actions, it is impossible to have a CER greater than 0.54214, so an exception is raised in this case.</p> required <code>actions</code> <code>tuple[str, ...]</code> <p>list of actions to use to add noise. Available actions are insert, swap, substitute and delete. Defaults to <code>[insert, swap, substitute, delete]</code>.</p> <code>_AVAILABLE_ACTIONS</code> <code>character_set</code> <code>tuple[str, ...]</code> <p>set of characters from which character will be drawn for insert or substitute actions. Defaults to string.ascii_letters.</p> <code>tuple(ascii_letters)</code> <code>seed</code> <code>int | None</code> <p>A seed to ensure reproducibility. Defaults to <code>None</code>.</p> <code>None</code> <code>natural_language_swap_correction</code> <code>float</code> <p>A correction factor to take into account the fact that natural language is not random. Defaults to 1.052, which is the correction factor for English.</p> <code>1.052</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the action is not one of the available actions.</p> Source code in <code>textnoisr/noise.py</code> <pre><code>class CharNoiseAugmenter:\n    r\"\"\"Add noise into text according to a noise level measured between 0 and 1.\n\n    It will add noise to a string by modifying each character\n        according to a probability and a list of actions.\n        Possible actions are `insert`, `swap`, `substitute` and `delete`.\n\n    For actions `insert` and `substitute`, new characters are drawn from `character_set`\n        which is the set of ascii letters (lower and upper) by default.\n    The `swap` action swaps 2 consecutive characters,\n        but **one character can not be swapped twice**.\n        So if a pair of characters has been swapped,\n        we move to the next pair of characters.\n\n    With enough samples, the CER of the output tends to the noise level\n        (terms and conditions may apply,\n        see details in [docs/how_this_works.md](how_this_works.md)).\n\n    Args:\n        noise_level: between 0 and 1, it corresponds to the level of noise to add to the\n            text. In most cases (see details above for caveats),\n            the Character Error Rate of the output will converge to this value.\n            For `swap` actions, it is impossible to have a CER greater than 0.54214,\n            so an exception is raised in this case.\n        actions: list of actions to use to add noise. Available actions are *insert*,\n            *swap*, *substitute* and *delete*.\n            Defaults to `[insert, swap, substitute, delete]`.\n        character_set: set of characters from which character will be drawn for\n            *insert* or *substitute* actions. Defaults to string.ascii_letters.\n        seed: A seed to ensure reproducibility.\n            Defaults to `None`.\n        natural_language_swap_correction: A correction factor to take into account the\n            fact that natural language is not random.\n            Defaults to 1.052, which is the correction factor for English.\n\n    Raises:\n             ValueError: If the action is not one of the available actions.\n    \"\"\"\n\n    _AVAILABLE_ACTIONS = (\"insert\", \"swap\", \"substitute\", \"delete\")\n\n    def __init__(\n        self,\n        noise_level: float,\n        actions: tuple[str, ...] = _AVAILABLE_ACTIONS,\n        character_set: tuple[str, ...] = tuple(string.ascii_letters),\n        seed: int | None = None,\n        natural_language_swap_correction: float = 1.052,\n    ) -&gt; None:\n        self.actions = [\n            x for i, x in enumerate(actions) if x not in actions[:i]\n        ]  # To avoid using list(set(actions))\n        self.character_set = character_set\n        self.noise_level = noise_level\n        self.random = random.Random(seed)  # nosec\n        self.natural_language_swap_correction = natural_language_swap_correction\n\n        # checks\n        unsupported_actions = [\n            a for a in self.actions if a not in CharNoiseAugmenter._AVAILABLE_ACTIONS\n        ]\n        if unsupported_actions:\n            raise ValueError(\n                f\"You provide unsupported actions: {unsupported_actions}. Available\"\n                f\" actions are {CharNoiseAugmenter._AVAILABLE_ACTIONS}\"\n            )\n        if not 0 &lt;= self.noise_level &lt;= 1:\n            raise ValueError(\n                \"Noise level must be between 0 and 1 (included), you provide\"\n                f\" {self.noise_level}\"\n            )\n        if (\n            self.noise_level\n            &gt; unbias.MAX_SWAP_LEVEL / self.natural_language_swap_correction\n        ) &amp; (\"swap\" in self.actions):\n            raise ValueError(\n                \"You cannot have a CER greater than\"\n                f\" {unbias.MAX_SWAP_LEVEL / self.natural_language_swap_correction} when\"\n                \" using action `swap`\"\n            )\n\n    def _random_success(self, p: float) -&gt; bool:\n        \"\"\"Determine whether a random event is successful based on a probability value.\n\n        Args:\n            p: The probability value for the random event (must be between 0 and 1).\n\n        Returns:\n            True with probability `p`, False otherwise.\n        \"\"\"\n        return self.random.random() &lt; p  # nosec\n\n    def _random_char(self, p: float, character_set: tuple[str, ...]) -&gt; str:\n        \"\"\"Return a random character with probability `p`, or an empty string.\n\n        Args:\n            p: A value between 0 and 1 representing the probability to return a random\n                character\n            character_set: A character set, for `random.choice()` to choose from.\n\n        Returns:\n            A random character with probability `p`, or an empty string.\n        \"\"\"\n        return self._random_success(p) * self.random.choice(character_set)  # nosec\n\n    def insert_random_chars(self, text: str, p: float) -&gt; str:\n        \"\"\"Insert random characters into a string.\n\n        For each character in the input string, a random character is inserted after it\n        with probability `p`. The random characters are chosen from self.character_set.\n\n        Args:\n            text: The input string to be modified.\n            p: probability to insert random char\n\n        Returns:\n            A string derived from `text` with random characters potentially inserted\n                after each character.\n        \"\"\"\n        return \"\".join(char + self._random_char(p, self.character_set) for char in text)\n\n    def _choose_another_character(self, char):\n        other_char = self.random.choice(self.character_set)\n        while other_char == char:\n            other_char = self.random.choice(self.character_set)\n        return other_char\n\n    def substitute_random_chars(self, text: str, p: float) -&gt; str:\n        \"\"\"Substitute random characters of a string.\n\n        Each character of the input string is substituted with another random one with\n        probability `p`. The random characters are chosen from the self.character_set.\n\n        Args:\n            text: The input string to be modified.\n            p: probability to substitute a character\n\n        Returns:\n            A string derived from `text` with potentially substituted characters.\n        \"\"\"\n        return \"\".join(\n            self._choose_another_character(char) if self._random_success(p) else char\n            for char in text\n        )\n\n    def delete_random_chars(self, text: str, p: float) -&gt; str:\n        \"\"\"Delete random characters of a string.\n\n        Each character of the input string is deleted with another random one with\n        probability `p`.\n\n        Args:\n            text: The input string to be modified.\n            p: probability to delete random character\n\n        Returns:\n            A string derived from `text` with potentially deleted characters.\n        \"\"\"\n        return \"\".join([\"\" if self._random_success(p) else char for char in text])\n\n    def consecutive_swap_random_chars(self, text: str, p: float) -&gt; str:\n        \"\"\"Swap random consecutive characters of a string.\n\n        Each character of the input string is swapped with the next one with\n        a probability linked to `p` (i.e. after a unbiasing).\n        Notice that a character can only be swapped once.\n\n        Args:\n            text: The input string to be modified.\n            p: probability for a character to be swapped.\n                It is modified for the CER of the result to converge to this value.\n\n        Returns:\n            A string derived from `text` with potentially swapped characters.\n        \"\"\"\n        p = unbias.unbias_swap(p, len(text), self.natural_language_swap_correction)\n\n        result = []\n        was_swapped = False\n        for current_char, next_char in zip_longest(text, text[1:], fillvalue=\"\"):\n            if not was_swapped:\n                if self._random_success(p):\n                    result.extend([next_char, current_char])\n                    was_swapped = True\n                    continue\n                result.append(current_char)\n            was_swapped = False\n        return \"\".join(result)\n\n    def add_noise(self, text: str | list[str]) -&gt; str | list[str]:\n        \"\"\"Add noise to a text. The text can be splitted into words.\n\n        Args:\n            text: The text on which to add noise.\n\n        Returns:\n            The text with noise.\n        \"\"\"\n        if isinstance(text, list):\n            p = unbias.unbias_split_into_words(self.noise_level, text)\n        else:\n            p = self.noise_level\n\n        p_effective = unbias.unbias_several_action(p, len(self.actions))\n\n        for action in self.actions:\n            match action:\n                case \"insert\":\n                    action_function = self.insert_random_chars\n                case \"swap\":\n                    action_function = self.consecutive_swap_random_chars\n                case \"substitute\":\n                    action_function = self.substitute_random_chars\n                case \"delete\":\n                    action_function = self.delete_random_chars\n                case _:\n                    raise ValueError(\n                        \"Action should be one of\"\n                        f\" {CharNoiseAugmenter._AVAILABLE_ACTIONS!r}\"\n                    )\n\n            if isinstance(text, list):\n                text = [action_function(word, p_effective) for word in text]\n            else:\n                text = action_function(text, p_effective)\n        return text\n</code></pre>"},{"location":"api/#textnoisr.noise.CharNoiseAugmenter.add_noise","title":"<code>add_noise(text)</code>","text":"<p>Add noise to a text. The text can be splitted into words.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | list[str]</code> <p>The text on which to add noise.</p> required <p>Returns:</p> Type Description <code>str | list[str]</code> <p>The text with noise.</p> Source code in <code>textnoisr/noise.py</code> <pre><code>def add_noise(self, text: str | list[str]) -&gt; str | list[str]:\n    \"\"\"Add noise to a text. The text can be splitted into words.\n\n    Args:\n        text: The text on which to add noise.\n\n    Returns:\n        The text with noise.\n    \"\"\"\n    if isinstance(text, list):\n        p = unbias.unbias_split_into_words(self.noise_level, text)\n    else:\n        p = self.noise_level\n\n    p_effective = unbias.unbias_several_action(p, len(self.actions))\n\n    for action in self.actions:\n        match action:\n            case \"insert\":\n                action_function = self.insert_random_chars\n            case \"swap\":\n                action_function = self.consecutive_swap_random_chars\n            case \"substitute\":\n                action_function = self.substitute_random_chars\n            case \"delete\":\n                action_function = self.delete_random_chars\n            case _:\n                raise ValueError(\n                    \"Action should be one of\"\n                    f\" {CharNoiseAugmenter._AVAILABLE_ACTIONS!r}\"\n                )\n\n        if isinstance(text, list):\n            text = [action_function(word, p_effective) for word in text]\n        else:\n            text = action_function(text, p_effective)\n    return text\n</code></pre>"},{"location":"api/#textnoisr.noise.CharNoiseAugmenter.consecutive_swap_random_chars","title":"<code>consecutive_swap_random_chars(text, p)</code>","text":"<p>Swap random consecutive characters of a string.</p> <p>Each character of the input string is swapped with the next one with a probability linked to <code>p</code> (i.e. after a unbiasing). Notice that a character can only be swapped once.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input string to be modified.</p> required <code>p</code> <code>float</code> <p>probability for a character to be swapped. It is modified for the CER of the result to converge to this value.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string derived from <code>text</code> with potentially swapped characters.</p> Source code in <code>textnoisr/noise.py</code> <pre><code>def consecutive_swap_random_chars(self, text: str, p: float) -&gt; str:\n    \"\"\"Swap random consecutive characters of a string.\n\n    Each character of the input string is swapped with the next one with\n    a probability linked to `p` (i.e. after a unbiasing).\n    Notice that a character can only be swapped once.\n\n    Args:\n        text: The input string to be modified.\n        p: probability for a character to be swapped.\n            It is modified for the CER of the result to converge to this value.\n\n    Returns:\n        A string derived from `text` with potentially swapped characters.\n    \"\"\"\n    p = unbias.unbias_swap(p, len(text), self.natural_language_swap_correction)\n\n    result = []\n    was_swapped = False\n    for current_char, next_char in zip_longest(text, text[1:], fillvalue=\"\"):\n        if not was_swapped:\n            if self._random_success(p):\n                result.extend([next_char, current_char])\n                was_swapped = True\n                continue\n            result.append(current_char)\n        was_swapped = False\n    return \"\".join(result)\n</code></pre>"},{"location":"api/#textnoisr.noise.CharNoiseAugmenter.delete_random_chars","title":"<code>delete_random_chars(text, p)</code>","text":"<p>Delete random characters of a string.</p> <p>Each character of the input string is deleted with another random one with probability <code>p</code>.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input string to be modified.</p> required <code>p</code> <code>float</code> <p>probability to delete random character</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string derived from <code>text</code> with potentially deleted characters.</p> Source code in <code>textnoisr/noise.py</code> <pre><code>def delete_random_chars(self, text: str, p: float) -&gt; str:\n    \"\"\"Delete random characters of a string.\n\n    Each character of the input string is deleted with another random one with\n    probability `p`.\n\n    Args:\n        text: The input string to be modified.\n        p: probability to delete random character\n\n    Returns:\n        A string derived from `text` with potentially deleted characters.\n    \"\"\"\n    return \"\".join([\"\" if self._random_success(p) else char for char in text])\n</code></pre>"},{"location":"api/#textnoisr.noise.CharNoiseAugmenter.insert_random_chars","title":"<code>insert_random_chars(text, p)</code>","text":"<p>Insert random characters into a string.</p> <p>For each character in the input string, a random character is inserted after it with probability <code>p</code>. The random characters are chosen from self.character_set.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input string to be modified.</p> required <code>p</code> <code>float</code> <p>probability to insert random char</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string derived from <code>text</code> with random characters potentially inserted after each character.</p> Source code in <code>textnoisr/noise.py</code> <pre><code>def insert_random_chars(self, text: str, p: float) -&gt; str:\n    \"\"\"Insert random characters into a string.\n\n    For each character in the input string, a random character is inserted after it\n    with probability `p`. The random characters are chosen from self.character_set.\n\n    Args:\n        text: The input string to be modified.\n        p: probability to insert random char\n\n    Returns:\n        A string derived from `text` with random characters potentially inserted\n            after each character.\n    \"\"\"\n    return \"\".join(char + self._random_char(p, self.character_set) for char in text)\n</code></pre>"},{"location":"api/#textnoisr.noise.CharNoiseAugmenter.substitute_random_chars","title":"<code>substitute_random_chars(text, p)</code>","text":"<p>Substitute random characters of a string.</p> <p>Each character of the input string is substituted with another random one with probability <code>p</code>. The random characters are chosen from the self.character_set.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input string to be modified.</p> required <code>p</code> <code>float</code> <p>probability to substitute a character</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string derived from <code>text</code> with potentially substituted characters.</p> Source code in <code>textnoisr/noise.py</code> <pre><code>def substitute_random_chars(self, text: str, p: float) -&gt; str:\n    \"\"\"Substitute random characters of a string.\n\n    Each character of the input string is substituted with another random one with\n    probability `p`. The random characters are chosen from the self.character_set.\n\n    Args:\n        text: The input string to be modified.\n        p: probability to substitute a character\n\n    Returns:\n        A string derived from `text` with potentially substituted characters.\n    \"\"\"\n    return \"\".join(\n        self._choose_another_character(char) if self._random_success(p) else char\n        for char in text\n    )\n</code></pre>"},{"location":"api/#textnoisr.noise_dataset","title":"<code>textnoisr.noise_dataset</code>","text":"<p>Noise a NLP dataset.</p>"},{"location":"api/#textnoisr.noise_dataset.add_noise","title":"<code>add_noise(dataset, noise_augmenter, feature_name='tokens', **kwargs)</code>","text":"<p>Add random noise to dataset items.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>dataset containing texts</p> required <code>noise_augmenter</code> <code>CharNoiseAugmenter</code> <p>noise augmenter from module <code>textprocessing.noise</code> to use to perform noise data augmentation</p> required <code>feature_name</code> <code>str</code> <p>The name of the dataset feature (column name) on which to add noise (usually \"tokens\" or \"text\")</p> <code>'tokens'</code> <code>**kwargs</code> <code>Any</code> <p>refers to huggingface dataset.map() argument, see github.com/huggingface/datasets/blob/main/src/datasets/arrow_dataset.py</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>noised dataset</p> Source code in <code>textnoisr/noise_dataset.py</code> <pre><code>def add_noise(\n    dataset: Dataset,\n    noise_augmenter: noise.CharNoiseAugmenter,\n    feature_name: str = \"tokens\",\n    **kwargs: Any,\n) -&gt; Dataset:\n    \"\"\"Add random noise to dataset items.\n\n    Args:\n        dataset: dataset containing texts\n        noise_augmenter: noise augmenter from module `textprocessing.noise` to use to\n            perform noise data augmentation\n        feature_name: The name of the dataset feature (column name) on which to add\n            noise (usually \"tokens\" or \"text\")\n        **kwargs: refers to huggingface dataset.map() argument, see\n            github.com/huggingface/datasets/blob/main/src/datasets/arrow_dataset.py\n\n    Returns:\n        noised dataset\n    \"\"\"\n    return dataset.map(\n        lambda x: _add_noise_to_example(x, noise_augmenter, feature_name), **kwargs\n    )\n</code></pre>"},{"location":"api/#textnoisr.noise_unbiasing","title":"<code>textnoisr.noise_unbiasing</code>","text":"<p>Unbias noise.</p> <p>See this document.</p>"},{"location":"api/#textnoisr.noise_unbiasing.__compute_expected_cer_from_noise_level","title":"<code>__compute_expected_cer_from_noise_level(p, N)</code>","text":"<p>Compute the expected CER from an uncorrected (biased) p for a string of length N.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float</code> <p>Uncorrected (biased) noise level, that is probability to swap an unswapped character</p> required <code>N</code> <code>int</code> <p>The length of the string.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The expected CER (in the sense of the expected value)</p> Source code in <code>textnoisr/noise_unbiasing.py</code> <pre><code>def __compute_expected_cer_from_noise_level(p: float, N: int) -&gt; float:\n    \"\"\"Compute the expected CER from an uncorrected (biased) p for a string of length N.\n\n    Args:\n        p: Uncorrected (biased) noise level,\n            that is probability to swap an unswapped character\n        N: The length of the string.\n\n    Returns:\n        The expected CER (in the sense of the\n            [expected value](https://en.wikipedia.org/wiki/Expected_value))\n    \"\"\"\n    p = float(p)\n    q = 1 - p\n    # The transition matrix of the Markov Chain:\n    P = np.array(\n        [\n            [0, 0, 1, 0, 0, 0, 0, 0],\n            [0, 0, 0, p, q, 0, 0, 0],\n            [p, q, 0, 0, 0, 0, 0, 0],\n            [0, 0, 1, 0, 0, 0, 0, 0],\n            [0, 0, 0, p, q, 0, 0, 0],\n            [0, 0, 1, 0, 0, 0, 0, 0],\n            [0, 0, 0, p, q, 0, 0, 0],\n            [0, 0, 0, 0, 0, p, q, 0],\n        ]\n    )\n    # Initialize the state:\n    state = SWAP_START_STATE\n    # Initialize the value of Levenshtein (this value should be zero):\n    levenshtein = state @ SWAP_LEVENSHTEIN_VALUE\n\n    for _ in range(N - 1):\n        # We compute the probability distribution of the Markov Chain\n        # for the next iteration:\n        state = state @ P\n        levenshtein += state @ SWAP_LEVENSHTEIN_VALUE\n    return levenshtein / N\n</code></pre>"},{"location":"api/#textnoisr.noise_unbiasing.__compute_noise_level_from_expected_cer","title":"<code>__compute_noise_level_from_expected_cer(cer, N)</code>  <code>cached</code>","text":"<p>Compute the noise level we have to pass as input in order to get.</p> <p>This is the real \"unbias_swap\" function. The other one is just a wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>cer</code> <code>float</code> <p>The Character Error Rate we want to have.</p> required <code>N</code> <code>int</code> <p>The length of the string.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Unbiased probability</p> Source code in <code>textnoisr/noise_unbiasing.py</code> <pre><code>@functools.cache\ndef __compute_noise_level_from_expected_cer(cer: float, N: int) -&gt; float:\n    \"\"\"Compute the noise level we have to pass as input in order to get.\n\n    This is the real \"unbias_swap\" function. The other one is just a wrapper.\n\n    Args:\n        cer: The Character Error Rate we want to have.\n        N: The length of the string.\n\n    Returns:\n        Unbiased probability\n    \"\"\"\n    return float(\n        scipy.optimize.fsolve(\n            lambda x: __compute_expected_cer_from_noise_level(float(x[0]), N) - cer,\n            [0],\n        )[0]\n    )\n</code></pre>"},{"location":"api/#textnoisr.noise_unbiasing.unbias_several_action","title":"<code>unbias_several_action(p, n_actions)</code>","text":"<p>Unbias probability to remove the bias due to the successive actions.</p> <p>If applied N times, the probability to change something will become.</p> <p>p_effective = (1 - (1 - noise_level) ** N)</p> <p>so we have to invert it to have p_effective = p.</p> <p>Notice that at first order of the Taylor expansion, this becomes p = p / len(self.actions)</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float</code> <p>Input probability. The user want the expectation of the Character Error Rate to tend to this value.</p> required <code>n_actions</code> <code>int</code> <p>Number of actions.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Unbiased probability</p> Source code in <code>textnoisr/noise_unbiasing.py</code> <pre><code>def unbias_several_action(p: float, n_actions: int) -&gt; float:\n    \"\"\"Unbias probability to remove the bias due to the successive actions.\n\n    If applied N times, the probability to change something will become.\n\n    p_effective = (1 - (1 - noise_level) ** N)\n\n    so we have to invert it to have p_effective = p.\n\n    Notice that at first order of the Taylor expansion, this becomes\n    p = p / len(self.actions)\n\n    Args:\n        p: Input probability. The user want the expectation of the Character Error Rate\n            to tend to this value.\n        n_actions: Number of actions.\n\n    Returns:\n        Unbiased probability\n    \"\"\"\n    p_effective: float = 1.0 - (1.0 - p) ** (1 / n_actions)\n    return p_effective\n</code></pre>"},{"location":"api/#textnoisr.noise_unbiasing.unbias_split_into_words","title":"<code>unbias_split_into_words(p, text)</code>","text":"<p>Unbias probability to take into account the absence of spaces when splitting.</p> <p>We need to apply noise word by word in order to have an output list with the same length as the input. If we applied noise word by word we will decrease the effective error rate since no noise will be added on spaces between words. So we increase the probability in order to compensate this loss</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float</code> <p>Input probability. The user want the expectation of the Character Error Rate to tend to this value.</p> required <code>text</code> <code>list[str]</code> <p>Text on which we unbias.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Unbiased probability</p> Source code in <code>textnoisr/noise_unbiasing.py</code> <pre><code>def unbias_split_into_words(p: float, text: list[str]) -&gt; float:\n    \"\"\"Unbias probability to take into account the absence of spaces when splitting.\n\n    We need to apply noise word by word in order to have an output list with the same\n    length as the input.\n    If we applied noise word by word we will decrease the effective error rate\n    since no noise will be added on spaces between words.\n    So we increase the probability in order to compensate this loss\n\n\n    Args:\n        p: Input probability. The user want the expectation of the Character Error Rate\n            to tend to this value.\n        text: Text on which we unbias.\n\n    Returns:\n        Unbiased probability\n    \"\"\"\n    n_chars = sum(map(len, text))\n    n_spaces = len(text) - 1\n    return p * (1 + n_spaces / n_chars)\n</code></pre>"},{"location":"api/#textnoisr.noise_unbiasing.unbias_swap","title":"<code>unbias_swap(p, N, natural_language_swap_correction)</code>  <code>cached</code>","text":"<p>Re-compute p to take unbiasing into account.</p> <p>See doc for more details.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float</code> <p>Input probability. The user want the expectation of the Character Error Rate to tend to this value.</p> required <code>N</code> <code>int</code> <p>The length of the string.</p> required <code>natural_language_swap_correction</code> <code>float</code> <p>A correction factor to take into account the fact that natural language is not random.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Unbiased probability, using an approximation formula for strings that are too long.</p> Source code in <code>textnoisr/noise_unbiasing.py</code> <pre><code>@functools.lru_cache\ndef unbias_swap(p: float, N: int, natural_language_swap_correction: float) -&gt; float:\n    \"\"\"Re-compute p to take unbiasing into account.\n\n    See doc for [more details](swap_unbiasing.md).\n\n    Args:\n        p: Input probability. The user want the expectation of the Character Error Rate\n            to tend to this value.\n        N: The length of the string.\n        natural_language_swap_correction: A correction factor to take into account the\n            fact that natural language is not random.\n\n    Returns:\n        Unbiased probability, using an approximation formula for strings that are too\n            long.\n    \"\"\"\n    # To avoid some \"math domain error\" later, in the case when a former unbiasing\n    # made p &gt; MAX_SWAP_LEVEL, we need to force it at the max level:\n    p = min(p, MAX_SWAP_LEVEL) * natural_language_swap_correction\n    # Whatever, N = nchar = 0 anyway, so returning p or something else does not matter:\n    if N == 0:\n        return p\n    if p == 0:\n        return 0\n    # We have a formula if N is too long:\n    if N &gt; 50:\n        return (2 - p) / 2 - math.sqrt((p**2) - (8 * p) + 4) / 2\n    return __compute_noise_level_from_expected_cer(p, N)\n</code></pre>"},{"location":"how_this_works/","title":"How this works","text":""},{"location":"how_this_works/#high-level-design","title":"High level design","text":"<p>As can be seen in the source code, there are three main parts:</p> <ul> <li> <p>The module textnoisr/noise_dataset.py,     consists on wrapper functions to make this library works seamlessly on the <code>Dataset</code> class.</p> </li> <li> <p>The module textnoisr/noise.py     contains a class <code>CharNoiseAugmenter</code> that work on the level of a single document.     This class is basically a wrapper around four methods (one for each action).     In pseudo-python, the first three (\"delete\", \"insert\", \"substitute\") are one-liners like     <pre><code>def ACTION_random_chars(text: str, p: float) -&gt; str:\n    return \"\".join(DO_STUFF(char) if SOME_CONDITION for char in text)\n</code></pre>     It is a little bit more complicated for the \"swap\" action, since we do not want     two non-consecutive characters to be swapped. There are two effects to this detail:</p> <ul> <li>the code is a little bit more convoluted for \"swap\" (a dozen of lines instead of two), in order to avoid swapping again a character that has already been swapped.</li> <li>this introduces a bias: the Character Error Rate for the case of \"swap\" does not tends to the noise level anymore. We need to unbias the noise level beforehand in the \"swap\" case. This is taken into account in the last module.</li> </ul> </li> <li> <p>The module     textnoisr/noise_unbiasing.py,     hides the details needed for the Character Error Rate to tend to the noise level.</p> </li> </ul>"},{"location":"how_this_works/#advanced-understanding-the-unbiasing","title":"Advanced: understanding the unbiasing","text":"<p>As previously said, one important feature of this module is that we want the noise level to be compatible with the notion of Character Error Rate. More precisely, we want (as far as possible) for the expected value of the Character Error Rate of the output to be the <code>noise_level</code> given as input.</p> <p>Several aspects have been taken into account to enforce this behavior, and several biases have been removed.</p>"},{"location":"how_this_works/#actions-applied-successively","title":"Actions applied successively","text":"<p>When actions are applied successively to the whole text, the text will be processed several times. If applied \\(N\\) times with probability \\(p\\), the total probability \\(P\\) to change something will become</p> \\[ P = 1 - (1 - p) ^ N.\\] <p>In our case, \\(N =\\mathtt{len(actions)}\\) and \\(p =\\mathtt{noise\\_level}\\), so that</p> \\[ \\mathtt{effective\\_noise\\_level} = 1 - (1 - \\mathtt{noise\\_level}) ^ \\mathtt{len(actions)} .\\] <p>We have to modify the input \\(\\mathtt{noise\\_level}\\) in order to get get the expected \\(\\mathtt{effective\\_noise\\_level}\\):</p> \\[ \\mathtt{noise\\_level} \\leftarrow 1.0 - (1.0 - \\mathtt{noise\\_level}) ^ {1 / \\mathtt{len(actions)}}\\]"},{"location":"how_this_works/#list-of-words","title":"List of words","text":"<p>Note that the <code>CharNoiseAugmenter</code> can add noise to text in the form of list of     words instead of single string. In that case, we need to apply noise word by word     in order to have an output list with the same length as the input. If we applied noise word by word we will decrease the effective Character Error Rate of the whole string     since no noise will be added on spaces between words.     So we increase the probability in order to compensate this loss:</p> \\[ \\mathtt{noise\\_level} \\leftarrow  \\mathtt{noise\\_level}  \\times (1 + n_{spaces} / n_{chars}) \\] <p>where \\(n_{chars}\\) is the sum of the number of characters in each word, and \\(n_{spaces}\\) the number     of spaces between words in the string.</p>"},{"location":"how_this_works/#action-delete","title":"Action <code>delete</code>","text":"<p>No bias to be corrected here.</p>"},{"location":"how_this_works/#action-insert","title":"Action <code>insert</code>","text":"<p>No bias to be corrected here.</p>"},{"location":"how_this_works/#action-substitute","title":"Action <code>substitute</code>","text":"<p>No bias to be corrected here, since we ensure that a character is not substituted by itself.</p>"},{"location":"how_this_works/#action-swap","title":"Action <code>swap</code>","text":"<p>Huge bias to be corrected here.</p> <p>TL;DR: A correction using Markov Chains has been implemented     for the Character Error Rate to converge to <code>noise_level</code>.     An extra adjustment factor is then applied     to take into account the structured pattern of natural language.</p> <p>If you want to know the gory details, you may want to check this dedicated document.</p>"},{"location":"how_this_works/#conclusion","title":"Conclusion","text":"<p>What works</p> <p>The implementation of the <code>CharNoiseAugmenter</code> takes all these six aspects into account, so the user just have to pass the <code>noise_level</code> she/he expects the Character Error Rate to be at the end.</p> <p>Warning</p> <p>Be aware that some effects may still remain:</p> <ul> <li>When using <code>[\"delete\", \"insert\"]</code>, it is possible to delete a character     and substitute it with the very same one, resulting in no error where two are naively expected.</li> <li>For very high noise level,     the computation of the Character Error Rate may not reflect the actual actions performed.</li> <li>A lot of these results is based on the assumptions that the character set is very large.     This does not really hold for real words: for example,     swapping the last two characters of <code>all</code> results in no change.</li> </ul> <p>Overall, the unit tests show that on a real-world dataset for a noise level of 10%,     the absolute error between the Character Error Rate and the noise level is less than one percentage point.</p>"},{"location":"results/","title":"Results","text":""},{"location":"results/#result-in-the-ideal-case-of-a-string-with-no-repeating-characters","title":"Result in the ideal case of a string with no repeating characters","text":"<p>As detailed theoretically here, the four actions have arbitrarly small errors on texts where each letter is unique, like <code>\"01234\"</code> or <code>\"abcdefghijklmnopqrstuvwxyz\"</code>.</p> <p>This has been extensively unit-tested in <code>textnoisr/tests/textnoisr/test_noise.py::test__atomic_random_chars</code> :</p> <pre><code>...\naverage_cer = cer.compute(predictions=..., references=...)\n...\nassert isclose(average_cer, expected_average_cer, abs_tol=ABS_TOLERANCE, rel_tol=REL_TOLERANCE)\n</code></pre> <p>For arbitrary <code>ABS_TOLERANCE</code> and <code>REL_TOLERANCE</code>, we are guaranteed that there exists a large enough <code>n_sample</code> so that the assertion will not break.</p>"},{"location":"results/#benchmark-results","title":"Benchmark results","text":"<p>That being said, it is interesting to see wether that holds for natural language as well. For example, the fact that consecutive letters could be equal may lower the Character Error Rate for <code>swap</code> action.</p> <p><code>textnoisr</code> deals with a problem that is partly answered by nlpaug, that has a much larger scope (i.e. NLP augmentation in a broad sense) and does not primarly focus on correctness w.r.t Character Error Rate.</p> <p>In this section, we present the results of our algorithm, and compare them to the results of <code>nlpaug</code>.</p> <p>We want to check both</p> <ul> <li>on the left panel: the correctness of our approach. The closest to the diagonal, the better it is. This correspond to an absence of bias between the input and the output.</li> <li>on the right panel: how long it takes to execute the code.</li> </ul> <p> </p> <p> </p> <p> </p> <p> </p>"},{"location":"swap_unbiasing/","title":"Understanding swap unbiasing","text":""},{"location":"swap_unbiasing/#understanding-why-we-need-to-unbias-the-swap-action","title":"Understanding why we need to unbias the <code>swap</code> action","text":""},{"location":"swap_unbiasing/#naive-approach","title":"Naive approach","text":"<p>A naive approach to swapping consecutive characters would be the following one:</p> <pre><code>import random\n\n\ndef naive_swap_random_chars(text: str, p: float) -&gt; str:\n    char_list = list(text)\n    for i in range(len(char_list) - 1):\n        # Swap the current character with the next one with probability p\n        if random.random() &lt; p:\n            char_list[i], char_list[i + 1] = char_list[i + 1], char_list[i]\n    return \"\".join(char_list)\n</code></pre> <p>But the problem here is that one character can be swapped several time in a row, like the character <code>0</code> in the following example:</p> <p><pre><code>&gt;&gt;&gt; naive_swap_random_chars(\"0123456789\", 0.25)\n'1203457689'\n</code></pre> In the worst case scenario, only the first character is swapped and \"percolates\" from the beginning to the end:</p> <pre><code>&gt;&gt;&gt; naive_swap_random_chars(\"0123456789\", 1)\n'1234567890'\n</code></pre> <p>This not what is expected when \"swapping consecutive characters\".</p>"},{"location":"swap_unbiasing/#less-naive-approach","title":"Less Naive approach","text":"<p>To correct for this problem, we implement an approach that prevents swapping the current character if it has already been swapped.</p> <p>The function <code>naive_swap_random_chars</code> could be modified as following<sup>1</sup></p> <p><pre><code>import random\n\n\ndef less_naive_swap_random_chars(text: str, p: float) -&gt; str:\n    char_list = list(text)\n    swapped_indices = set()\n    for i in range(len(char_list) - 1):\n        if (random.random() &lt; p and\n            i not in swapped_indices):\n            char_list[i], char_list[i + 1] = char_list[i + 1], char_list[i]\n            swapped_indices.add(i + 1)\n    return \"\".join(char_list)\n</code></pre> Its behavior is consistent with what is expected: when given a probability of 1, it swaps the first two letters, then the two following ones, and so on. <pre><code>&gt;&gt;&gt; less_naive_swap_random_chars(\"0123456789\", 1)\n'1032547698'\n</code></pre></p> <p>Unfortunately (and contrary to the case of actions <code>delete</code>, <code>substitute</code> and <code>add</code>) there is no easy link between the noise level given as input, and Character Error Rate of the output.</p> <p>This is easily seen on the \"calibration curve\" of a <code>less_naive_swap_random_chars(text, p)</code> function, giving the CER with respect to input noise level <code>p</code>: </p> <p>This curve was obtained running <code>less_naive_swap_random_chars</code> on the string <code>\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"</code> 10 000 times for every percent of noise level. One can observe that it is not even monotonous, and that is does not reach 100%.</p> <p>One \"argument by hand-waving\" explains why this function is not the identity, as for <code>delete</code>, <code>substitute</code> and <code>add</code>: preventing swapping the current character if it has already been swapped introduces a correlation between two consecutive characters. This makes the Central Limit Theorem irrelevant to compute Character Error Rate from the probability to swap a single character.</p> <p>The actual approach we are using consists on getting the theoretical formula of this relationship between Character Error Rate and noise level, and plug it in the function that prevent swapping consecutive character <code>less_naive_swap_random_chars</code>.</p> <pre><code>def consecutive_swap_random_chars(text: str, p: float) -&gt; str:\n    p = unbias.unbias_swap(p, len(text))\n    return less_naive_swap_random_chars(text, p)\n</code></pre> <p>Let's study the impact of not swapping a character twice on the Character Error Rate!</p>"},{"location":"swap_unbiasing/#modelization-of-the-problem","title":"Modelization of the problem","text":""},{"location":"swap_unbiasing/#character-error-rate-as-normalized-levenshtein-distance","title":"Character Error Rate as \"normalized Levenshtein distance\"","text":"<p>The Character Error Rate is the edit distance between two strings divided by the length of the reference string. In this case, the \"edit distance\" is the Levenshtein distance, that is</p> <ul> <li>inserting a random character, e.g.    STEAM -&gt; STREAM,</li> <li>delete a random character, e.g.        STEAM  \u2192  TEAM,</li> <li>substituting a random character, e.g. STEAM -&gt; STEAL.</li> </ul> <p>Another \"argument by hand-waving\" explains the presence of a bias: swapping two (consecutive) characters is not stricto sensu an \"atomic action\" for Levenshtein distance<sup>2</sup>.</p> <p>We could claim that swapping two characters is equivalent to one deletion followed by one insertion, and that it should count as a distance of two. For example with 12345678 \u2194 12354678:</p> <p> 123 45678    &lt;-- 1 deletion 1 3245678    &lt;-- 1 insertion </p> <p>But this claim does not always hold. For example, two swaps 12345 678 \u2194 12354 768 can be treated as only three \"atomic\" operations, instead of four as naively expected:</p> <p> 1235476 8    &lt;-- 1 deletion 123 456 8    &lt;-- 1 substitution 123 45678    &lt;-- 1 insertion </p> <p>However, this is only possible because the two swap operation are contiguous.</p> <p>This observation<sup>3</sup> leads us to the following result:</p> <p>Contribution of individual swaps to total Levenshtein distance</p> <p>For each character of the resulting string, there are three possibilities</p> <ul> <li>The current character was not swapped: its contribution to the total Levenshtein distance is zero.</li> <li> <p>The current character was swapped, and the next-to-last<sup>4</sup> one was not: its contribution to the total Levenshtein distance is two. This is the naive case of 12354678, as well as the first swap of 12354 768, or the two swaps of 12345678 \u2194 21345768</p> </li> <li> <p>The current character was swapped, and the next-to-last<sup>4</sup> one was also swapped: its contribution to the total Levenshtein distance is only one. This is the case of the second swap of 12354 768.</p> </li> </ul>"},{"location":"swap_unbiasing/#modelization-of-swap-action-using-markov-chain","title":"Modelization of \"swap\" action using Markov Chain","text":"<p>As seen in the previous section, only the two last characters are useful to compute Character Error Rate. So let's represent the process of iterating through a string and swapping characters by a Markov Chain with states noted \\(XY\\), where</p> <ul> <li>\\(X\\) represents the status of previous swap transition,</li> <li>\\(Y\\) represents the status of the current swap transition.</li> </ul> <p>Those transitions are</p> <ul> <li>with probability \\(p\\), \"the swap was allowed, and effectively happened\",</li> <li>with probability \\(q = 1-p\\), \"the swap was allowed, but did not happen\",</li> <li>with probability \\(1\\), \"the swap was not allowed\".</li> </ul> <p>We represent the statuses as uppercase versions of the corresponding transition, so that \\(X\\) and \\(Y\\) can be \\(P\\), \\(Q\\) and \\(1\\). For the sake of completeness, we add a state \\(0\\) (and the states \\(0P\\) and \\(0Q\\)) to represent the very beginning of the string, when there is no \"previous swap\".</p> <p>The whole Markov Chain can be constructed by applying iteratively legal transitions from the state \\(0\\), and is given below: <pre><code>graph LR\n    1P((1P)) --1--&gt; P1((P1))\n    P1((P1)) --p--&gt; 1P((1P))\n    P1((P1)) --q--&gt; 1Q((1Q))\n    QP((QP)) --1--&gt; P1((P1))\n    1Q((1Q)) --q--&gt; QQ((QQ))\n    1Q((1Q)) --p--&gt; QP((QP))\n    QQ((QQ)) --p--&gt; QP((QP))\n    QQ((QQ)) --q--&gt; QQ((QQ))\n    OP --1--&gt; P1\n    O --q--&gt; OQ\n    OQ --p--&gt; QP\n    OQ --q--&gt; QQ\n    O --p--&gt; OP</code></pre></p> <p>Some facts are worth checking:</p> <ul> <li>Every \\(1\\) transition follows a \\(XP\\) state (i.e. \"The only possibility not to have a choice is to directly follow a swap\")</li> <li>Some states do not exists, like \\(11\\) (i.e. \"If the swap was not allowed, it is allowed again next iteration\") or \\(PQ\\) and \\(PP\\) (i.e. \"No random is at play when the previous transition was a swap, the only possibility is not to swap\").</li> <li>By construction, every arrow has the form</li> </ul> <pre><code>graph LR\n    XY((XY)) --z--&gt; YZ((YZ))</code></pre>"},{"location":"swap_unbiasing/#putting-it-together-computing-probability-of-levensthein-distance","title":"Putting it together: computing probability of Levensthein distance","text":"<p>Let's order the states of our Markov Chain to get a basis \\(B = (1P, 1Q, P1, QP, QQ, 0P, 0Q, 0)\\).</p> <p>In this basis,</p> <ul> <li>the vector that represent the contribution of each state to the total Levenshtein distance is \\(c = (1, 0, 0, 2, 0, 2, 0, 0)\\),</li> <li>the probability distribution at start is the vector \\(\\pi^{(0)} = (0, 0, 0, 0, 0, 0, 0, 1)\\), since we start from the starting state \\(0\\) with a probability of one,</li> <li>and the transition matrix is</li> </ul> \\[ P = \\begin{pmatrix} 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; p &amp; q &amp; 0 &amp; 0 &amp; 0\\\\ p &amp; q &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; p &amp; q &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; p &amp; q &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; p &amp; q &amp; 0 \\end{pmatrix}. \\] <p>After the \\(i\\)-th iteration, the probability distribution over the Markov Chain is given by \\(\\pi^{(i)} = \\pi^{(0)} P^i\\), and its expected contribution to Levenshtein distance is \\(c \\pi^{(0)} P^i\\).</p> <p>Finally, the total expected Levenshtein distance for a string of length \\(N\\) is<sup>5</sup></p> \\[\\mathtt{expected\\_levenshtein} = \\sum_{i=0}^{N-1} c \\pi^{(0)} P^i \\] <p>This quantity (up to the normalization factor \\(N\\)) is what is computed in the function <code>compute_expected_cer_from_noise_level</code>, and is then inverted using scipy in the function <code>compute_noise_level_from_expected_cer</code>.</p> <p>Result in the general case</p> <p>The <code>compute_noise_level_from_expected_cer</code> function is used to unbias the swap action.</p> <p>Notice that all of this is heavily cached to minimize computing.</p>"},{"location":"swap_unbiasing/#the-special-case-of-a-very-long-string","title":"The special case of a very long string","text":"<p>For a very long string, the expected Character Error Rate does not depend on the length of the string, but only on the input probability \\(p\\).</p> <p>In other words, the stationary distribution hypothesis stands, and the transitory states disappear:</p> <p><pre><code>graph LR\n    1P((1P)) --1--&gt; P1((P1))\n    P1((P1)) --p--&gt; 1P((1P))\n    P1((P1)) --q--&gt; 1Q((1Q))\n    QP((QP)) --1--&gt; P1((P1))\n    1Q((1Q)) --q--&gt; QQ((QQ))\n    1Q((1Q)) --p--&gt; QP((QP))\n    QQ((QQ)) --p--&gt; QP((QP))\n    QQ((QQ)) --q--&gt; QQ((QQ))</code></pre> With a truncated basis \\(B' = (1P, 1Q, P1, QP, QQ)\\), the \"individual contribution to Levenshtein\" vector becomes \\(c' = (1, 0, 0, 2, 0)\\), and the transition matrix is</p> \\[ P' = \\begin{pmatrix} 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; p &amp; q\\\\ p &amp; q &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; p &amp; q\\\\ \\end{pmatrix}. \\] <p>Let \\((x, y, z, t, u)\\) be the components of \\(\\pi\\) in the basis \\(B'\\). The stationary distribution hypothesis allows us to write for the stationary distribution \\(\\pi\\):</p> \\[\\pi = \\pi P', \\] <p>that corresponds to the linear system</p> \\[ \\left\\{ \\begin{array}{l} x = z \\times p \\\\ y = z \\times (1 - p) \\\\ z = x + t \\\\ t = (y + u) \\times p \\\\ 1 = x + y + z + u + t \\end{array}. \\right. \\] <p>The system is solved to find the value of \\(c' \\times \\pi^T = x + 2 \\times t\\):</p> \\[\\mathtt{expected\\_cer} = \\frac{2 \\times p - p ^ 2}{p + 1}\\] <p>This is exactly the curve we got experimentally with the \"calibration curve\" in the first section!</p> <p>We then inverse this to feed the function <code>unbias_swap</code>, in the case where the number of characters is greater than 50:</p> <p>Result in the special case of a very long string</p> \\[ p = \\frac{2 - \\mathtt{expected\\_cer}}{2} - \\frac{\\sqrt{(\\mathtt{expected\\_cer}^2) - (8 \\times \\mathtt{expected\\_cer}) + 4}}{2} \\] <p>As mentioned earlier, there is a maximum reachable in the Character Error Rate when applying only <code>swap</code> operations. This maximum is directly due to the constraint of preventing swapping the current character if it has already been swapped.</p>"},{"location":"swap_unbiasing/#taking-into-account-repetitions-of-characters-in-natural-language","title":"Taking into account repetitions of characters in natural language","text":"<p>In natural language, our assumption that two consecutive characters are always different is not always true. To take this into account, we have to introduce a small multiplicative coefficient to the probability \\(p\\).</p> <p>For English language, we computed this correction factor to be approximately 1.052, which is the default in <code>textnoisr</code>.</p> <ol> <li> <p>Our actual implementation is slightly different than this one.\u00a0\u21a9</p> </li> <li> <p>Notice that the Damerau-Levensthein distance does include <code>swap</code> as an atomic operation. Using this distance instead of Levensthein makes the bias more simple to compute. But the first \"argument by hand-waving\" still holds, so the result is not completely trivial. This is beyond the scope of this library.\u00a0\u21a9</p> </li> <li> <p>A formal proof can be given by induction with the Wagner-Fischer algorithm \u21a9</p> </li> <li> <p>In this case, the last character was not swapped, due to the aforementioned constraint preventing swapping the current character if it has already been swapped. That's why we have to look at the next-to-last character.\u00a0\u21a9\u21a9</p> </li> <li> <p>in the actual implementation, a <code>for</code> loop is preferred over exponentiation of matrices for performance issues.\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorial/","title":"Tutorial","text":"<p>The easiest way to use <code>textnoisr</code> is to apply it directly to a string. We also implemented a wrapper to use it on  datasets.</p>"},{"location":"tutorial/#add-noise-to-a-single-string","title":"Add noise to a single string","text":"<pre><code>&gt;&gt;&gt; from textnoisr import noise\n&gt;&gt;&gt; text = \"The duck-billed platypus (Ornithorhynchus anatinus) is a small mammal.\"\n&gt;&gt;&gt; augmenter = noise.CharNoiseAugmenter(noise_level=0.1)\n&gt;&gt;&gt; print(augmenter.add_noise(text))\nThe dhuck-biled plstypus Ornithorhnchus anatinus) is a smaJl mammal.\n</code></pre> <p>By default, all actions (i.e. <code>\"insert\"</code>, <code>\"swap\"</code>, <code>\"substitute\"</code>, <code>\"delete\"</code>) are applied to the sentence successively, but we can choose to apply only a subset of them:</p> <pre><code>&gt;&gt;&gt; augmenter = noise.CharNoiseAugmenter(noise_level=0.2, actions=[\"delete\"])\n&gt;&gt;&gt; print(augmenter.add_noise(text))\nThe dckilledplypus Ornithrhynhstinus is a mal mamal.\n</code></pre> <p>The Character Error Rate of the result should converge to the input value <code>noise_level</code>, although it is not obvious with only one sentence: the effect of the Law of Large Number will be easily seen in the next section dealing with whole datasets.</p>"},{"location":"tutorial/#add-noise-to-datasets","title":"Add noise to  datasets","text":"<p>For example, let's consider the rotten tomatoes dataset:</p> <pre><code>&gt;&gt;&gt; from datasets import load_dataset  # https://huggingface.co/docs/datasets\n&gt;&gt;&gt; dataset = load_dataset(\"rotten_tomatoes\", split=\"train\")\n&gt;&gt;&gt; print(dataset)\nDataset({\n    features: ['text', 'label'],\n    num_rows: 8530\n})\n</code></pre> <p>We want to add some noise to the <code>text</code> feature of this dataset. We use the <code>noise_level</code> parameter to control how much noisy will be the result:</p> <pre><code>&gt;&gt;&gt; from textnoisr.dataprep import noise_dataset\n&gt;&gt;&gt; from textnoisr import noise\n&gt;&gt;&gt;\n&gt;&gt;&gt; noised_dataset = noise_dataset.add_noise(\n&gt;&gt;&gt;      dataset,\n&gt;&gt;&gt;      noise.CharNoiseAugmenter(noise_level=0.1),\n&gt;&gt;&gt;      feature_name=\"text\",\n&gt;&gt;&gt; )\n&gt;&gt;&gt; print(f'{dataset[\"text\"][42]!r}')\n&gt;&gt;&gt; print(f'{noised_dataset[\"text\"][42]!r}')\n'fuller would surely have called this gutsy and at times exhilarating movie a great yarn .'\n'fCuller vould surely aJe clled tihs gutsyrndk att ies exhiglrdating mvoie a great yarBA .'\n</code></pre> <p>Let's compute the Character Error Rate of the resulting dataset, with respect to the original. As mentioned in the introduction, it is expected to be close to the input <code>noise_level</code>.</p> <pre><code>&gt;&gt;&gt; pred = [aug_doc[\"text\"] for aug_doc in noised_dataset]\n&gt;&gt;&gt; ref = [doc[\"text\"] for doc in dataset]\n&gt;&gt;&gt;\n&gt;&gt;&gt; from evaluate import load  # https://huggingface.co/docs/evaluate\n&gt;&gt;&gt;\n&gt;&gt;&gt; cer = load(\"cer\")\n&gt;&gt;&gt; print(f\"Character Error Rate = {cer.compute(predictions=pred, references=ref):.3f}\")\nCharacter Error Rate = 0.098\n</code></pre> <p>By default, all available actions are performed sequentially. It is possible to perform only some of the available actions though. Notice that for each of the five following tuples of actions, the Character Error Rate is close to the <code>noise_level</code>:</p> <pre><code>&gt;&gt;&gt; for actions in [\n&gt;&gt;&gt;     [\"delete\", \"insert\", \"substitute\", \"swap\"],\n&gt;&gt;&gt;     [\"delete\"],\n&gt;&gt;&gt;     [\"insert\"],\n&gt;&gt;&gt;     [\"substitute\"],\n&gt;&gt;&gt;     [\"swap\"],\n&gt;&gt;&gt; ]:\n&gt;&gt;&gt;     noised_dataset = noise_dataset.add_noise(\n&gt;&gt;&gt;         dataset,\n&gt;&gt;&gt;         noise.CharNoiseAugmenter(noise_level=0.01, actions=actions),\n&gt;&gt;&gt;         feature_name=\"text\",\n&gt;&gt;&gt;     )\n&gt;&gt;&gt;     pred = [aug_doc[\"text\"] for aug_doc in noised_dataset]\n&gt;&gt;&gt;     print(f\"-------\\nAction: {actions!r}\")\n&gt;&gt;&gt;     print(f'{noised_dataset[\"text\"][42]!r}')\n&gt;&gt;&gt;     print(f\"Character Error Rate = {cer.compute(predictions=pred, references=ref):.3f}\")\n-------\nAction: ('delete', 'insert', 'substitute', 'swap')\n'fuller would surely have called this gutsy and at timese xhilarating movie a great yarn .'\nCharacter Error Rate = 0.010\n-------\nAction: ('delete')\n'fuller would surely have called this gutsy and at timesexhilarating mvie a great yarn .'\nCharacter Error Rate = 0.010\n-------\nAction: ('insert')\n'fuller would surely have called this gutsy and at times exhilarating movie a great yarn .'\nCharacter Error Rate = 0.010\n-------\nAction: ('substitute')\n'fuller wouOd surely have called this gutsy and at times exhilarating movie v great yarn .'\nCharacter Error Rate = 0.010\n-------\nAction: ('swap')\n'fuller would surely have called this gutsy and at times exhliarating movie a great yarn .'\nCharacter Error Rate = 0.010\n</code></pre> <p>You can also use the <code>noise_dataset.add_noise</code> function on a dataset where text is splitted in words or tokens, that is a <code>list[str]</code> instead of a <code>str</code>.</p> <pre><code>&gt;&gt;&gt; def tokenization(example):  # Well, sort of...\n&gt;&gt;&gt;     example[\"tokens\"] = example[\"text\"].split()\n&gt;&gt;&gt;     return example\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; tokenized_dataset = dataset.map(tokenization, batched=False)\n&gt;&gt;&gt;\n&gt;&gt;&gt; noised_tokenized_dataset = noise_dataset.add_noise(\n&gt;&gt;&gt;     tokenized_dataset,\n&gt;&gt;&gt;     noise.CharNoiseAugmenter(noise_level=0.1, actions=actions),\n&gt;&gt;&gt;     feature_name=\"tokens\",\n&gt;&gt;&gt; )\n&gt;&gt;&gt; print(tokenized_dataset[42][\"tokens\"])\n&gt;&gt;&gt; print(noised_tokenized_dataset[42][\"tokens\"])\n['fuller', 'would', 'surely', 'have', 'called', 'this', 'gutsy', 'and', 'at', 'times', 'exhilarating', 'movie', 'a', 'great', 'yarn', '.']\n['fuller', 'woudl', 'surely', 'have', 'claled', 'this', 'gusty', 'and', 'at', 'times', 'exhilaratign', 'movie', 'a', 'great', 'yarn', '.']\n</code></pre> <p>Note</p> <p>It can be useful to split a dataset before applying noise to each token individually. This is the case when adding noise to a dataset already annotated for NER, while keeping the 1-1 mapping between annotations and words. In this case, you don't want spaces to be deleted, inserted, substituted, or swapped.</p>"}]}